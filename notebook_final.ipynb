{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network for Limit Order Books\n",
    "\n",
    "In this notebook, we implement the work of Zhang et al. We further improve this work by adding a self attention layer and by training the model on Order Flow data.\n",
    "\n",
    "### References:\n",
    "[1] Ntakaris A, Magris M, Kanniainen J, Gabbouj M, Iosifidis A. Benchmark dataset for mid‚Äêprice forecasting of limit order book data with machine learning methods. Journal of Forecasting. 2018 Dec;37(8):852-66. https://arxiv.org/abs/1705.03233\n",
    "\n",
    "[2] Zhang Z, Zohren S, Roberts S. DeepLOB: Deep convolutional neural networks for limit order books. IEEE Transactions on Signal Processing. 2019 Mar 25;67(11):3001-12. https://arxiv.org/abs/1808.03668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:53:32.068608Z",
     "iopub.status.busy": "2025-02-18T14:53:32.068356Z",
     "iopub.status.idle": "2025-02-18T14:53:37.453006Z",
     "shell.execute_reply": "2025-02-18T14:53:37.452310Z",
     "shell.execute_reply.started": "2025-02-18T14:53:32.068585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.isfile('data.zip'):\n",
    "    !wget https://github.com/Viccis33/DeepLOB_CNN_Order_Book/raw/refs/heads/master/data.zip\n",
    "    !unzip -n data.zip\n",
    "    print('Data downloaded.')\n",
    "else:\n",
    "    print('Data already existed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:53:37.454247Z",
     "iopub.status.busy": "2025-02-18T14:53:37.453804Z",
     "iopub.status.idle": "2025-02-18T14:53:37.510461Z",
     "shell.execute_reply": "2025-02-18T14:53:37.509532Z",
     "shell.execute_reply.started": "2025-02-18T14:53:37.454215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:53:56.931842Z",
     "iopub.status.busy": "2025-02-18T14:53:56.931277Z",
     "iopub.status.idle": "2025-02-18T14:53:56.938127Z",
     "shell.execute_reply": "2025-02-18T14:53:56.937203Z",
     "shell.execute_reply.started": "2025-02-18T14:53:56.931809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    dataY = dY[T - 1:N]\n",
    "\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX, dataY\n",
    "\n",
    "def torch_data(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    y = torch.from_numpy(y)\n",
    "    y = F.one_hot(y, num_classes=3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:53:58.891637Z",
     "iopub.status.busy": "2025-02-18T14:53:58.891316Z",
     "iopub.status.idle": "2025-02-18T14:53:58.897278Z",
     "shell.execute_reply": "2025-02-18T14:53:58.896439Z",
     "shell.execute_reply.started": "2025-02-18T14:53:58.891615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, k, num_classes, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "            \n",
    "        x = prepare_x(data)\n",
    "        y = get_label(data)\n",
    "        x, y = data_classification(x, y, self.T)\n",
    "        y = y[:,self.k] - 1\n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        self.x = torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used no auction dataset that is normalised by decimal precision approach in their work. The first seven days are training data and the last three days are testing data. A validation set (20%) from the training set is used to monitor the overfitting behaviours.\n",
    "\n",
    "The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information for a limit order book and we only use these 40 features in our network. The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons k (10,20,30,50,100).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:54:02.447116Z",
     "iopub.status.busy": "2025-02-18T14:54:02.446793Z",
     "iopub.status.idle": "2025-02-18T14:54:19.652025Z",
     "shell.execute_reply": "2025-02-18T14:54:19.651287Z",
     "shell.execute_reply.started": "2025-02-18T14:54:02.447091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 203800) (149, 50950) (149, 139587)\n"
     ]
    }
   ],
   "source": [
    "# please change the data_path to your local path\n",
    "# data_path = '/nfs/home/zihaoz/limit_order_book/data'\n",
    "\n",
    "dec_data = np.loadtxt('/kaggle/input/fi-2010/Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "dec_test1 = np.loadtxt('/kaggle/input/fi-2010/Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_test2 = np.loadtxt('/kaggle/input/fi-2010/Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "dec_test3 = np.loadtxt('/kaggle/input/fi-2010/Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "print(dec_train.shape, dec_val.shape, dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:54:19.653388Z",
     "iopub.status.busy": "2025-02-18T14:54:19.653086Z",
     "iopub.status.idle": "2025-02-18T14:54:29.899646Z",
     "shell.execute_reply": "2025-02-18T14:54:29.898809Z",
     "shell.execute_reply.started": "2025-02-18T14:54:19.653365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203701, 1, 100, 40]) torch.Size([203701])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "k1=2\n",
    "\n",
    "dataset_train = Dataset(data=dec_train, k=k1, num_classes=3, T=100)\n",
    "dataset_val = Dataset(data=dec_val, k=k1, num_classes=3, T=100)\n",
    "dataset_test = Dataset(data=dec_test, k=k1, num_classes=3, T=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:54:29.901508Z",
     "iopub.status.busy": "2025-02-18T14:54:29.901210Z",
     "iopub.status.idle": "2025-02-18T14:54:30.004873Z",
     "shell.execute_reply": "2025-02-18T14:54:30.004119Z",
     "shell.execute_reply.started": "2025-02-18T14:54:29.901484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3412, 0.0036, 0.3406,  ..., 0.0010, 0.3385, 0.0010],\n",
      "          [0.3412, 0.0036, 0.3406,  ..., 0.0010, 0.3390, 0.0032],\n",
      "          [0.3411, 0.0015, 0.3406,  ..., 0.0094, 0.3390, 0.0032],\n",
      "          ...,\n",
      "          [0.3413, 0.0068, 0.3408,  ..., 0.0010, 0.3396, 0.0040],\n",
      "          [0.3413, 0.0032, 0.3408,  ..., 0.0010, 0.3395, 0.0061],\n",
      "          [0.3413, 0.0032, 0.3408,  ..., 0.0010, 0.3395, 0.0061]]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([1.], dtype=torch.float64)\n",
      "torch.Size([1, 1, 100, 40]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\n",
    "\n",
    "for x, y in tmp_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeplob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:56:14.049517Z",
     "iopub.status.busy": "2025-02-18T14:56:14.049183Z",
     "iopub.status.idle": "2025-02-18T14:56:14.061118Z",
     "shell.execute_reply": "2025-02-18T14:56:14.060350Z",
     "shell.execute_reply.started": "2025-02-18T14:56:14.049493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class deeplob(nn.Module):\n",
    "    def __init__(self, y_len):\n",
    "        super().__init__()\n",
    "        self.y_len = y_len\n",
    "        \n",
    "        # convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        # inception moduels\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        \n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, self.y_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0: (number of hidden layers, batch size, hidden size)\n",
    "        h0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x_inp1 = self.inp1(x)\n",
    "        x_inp2 = self.inp2(x)\n",
    "        x_inp3 = self.inp3(x)  \n",
    "        \n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "        \n",
    "#         x = torch.transpose(x, 1, 2)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = torch.reshape(x, (-1, x.shape[1], x.shape[2]))\n",
    "        \n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        forecast_y = torch.softmax(x, dim=1)\n",
    "        \n",
    "        return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:56:14.315256Z",
     "iopub.status.busy": "2025-02-18T14:56:14.314970Z",
     "iopub.status.idle": "2025-02-18T14:56:14.330991Z",
     "shell.execute_reply": "2025-02-18T14:56:14.330214Z",
     "shell.execute_reply.started": "2025-02-18T14:56:14.315237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deeplob(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): Tanh()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 10), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp3): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lstm): LSTM(192, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deeplob(y_len = dataset_train.num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:56:16.194818Z",
     "iopub.status.busy": "2025-02-18T14:56:16.194446Z",
     "iopub.status.idle": "2025-02-18T14:56:19.603295Z",
     "shell.execute_reply": "2025-02-18T14:56:19.602647Z",
     "shell.execute_reply.started": "2025-02-18T14:56:16.194794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:56:19.604868Z",
     "iopub.status.busy": "2025-02-18T14:56:19.604377Z",
     "iopub.status.idle": "2025-02-18T14:56:19.613156Z",
     "shell.execute_reply": "2025-02-18T14:56:19.612339Z",
     "shell.execute_reply.started": "2025-02-18T14:56:19.604838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to encapsulate the training loop with early stopping\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs, patience=20):\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "    no_improve_epochs = 0  # Counter for early stopping\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to GPU\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Compute mean train loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        # Check for early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, './best_val_model_pytorch_30')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            no_improve_epochs = 0  # Reset counter\n",
    "            print('Model saved (New best validation loss)')\n",
    "        else:\n",
    "            no_improve_epochs += 1  # Increment counter\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {test_loss:.4f}, '\n",
    "              f'Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    " \n",
    "        # **Early stopping condition**\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f'Early stopping triggered after {it+1} epochs. No improvement for {patience} epochs.')\n",
    "            break  # Stop training\n",
    "\n",
    "    return train_losses[:it+1], test_losses[:it+1]  # Return only trained epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T14:56:20.162184Z",
     "iopub.status.busy": "2025-02-18T14:56:20.161878Z",
     "iopub.status.idle": "2025-02-18T16:51:23.018845Z",
     "shell.execute_reply": "2025-02-18T16:51:23.017928Z",
     "shell.execute_reply.started": "2025-02-18T14:56:20.162161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [01:21<2:41:25, 81.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 1/120, Train Loss: 1.0771, Validation Loss: 1.0753, Duration: 0:01:21.392893, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 2/120 [02:40<2:37:54, 80.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/120, Train Loss: 1.0796, Validation Loss: 1.0756, Duration: 0:01:19.519154, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñé         | 3/120 [03:59<2:35:22, 79.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 3/120, Train Loss: 1.0744, Validation Loss: 1.0671, Duration: 0:01:18.944226, Best Val Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 4/120 [05:19<2:34:01, 79.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 4/120, Train Loss: 1.0747, Validation Loss: 1.0666, Duration: 0:01:19.654634, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 5/120 [06:39<2:32:37, 79.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 5/120, Train Loss: 1.0735, Validation Loss: 1.0665, Duration: 0:01:19.570283, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 6/120 [07:59<2:31:29, 79.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/120, Train Loss: 1.0730, Validation Loss: 1.0680, Duration: 0:01:19.913628, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 7/120 [09:18<2:29:52, 79.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/120, Train Loss: 1.0727, Validation Loss: 1.0727, Duration: 0:01:19.262791, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 8/120 [10:38<2:28:48, 79.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/120, Train Loss: 1.0715, Validation Loss: 1.0669, Duration: 0:01:20.011234, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 9/120 [11:58<2:27:31, 79.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 9/120, Train Loss: 1.0717, Validation Loss: 1.0664, Duration: 0:01:19.810169, Best Val Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 10/120 [13:17<2:26:07, 79.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 10/120, Train Loss: 1.0701, Validation Loss: 1.0660, Duration: 0:01:19.612001, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 11/120 [14:38<2:25:13, 79.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 11/120, Train Loss: 1.0695, Validation Loss: 1.0650, Duration: 0:01:20.477976, Best Val Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 12/120 [15:57<2:23:44, 79.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/120, Train Loss: 1.0689, Validation Loss: 1.0663, Duration: 0:01:19.662416, Best Val Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 13/120 [17:17<2:22:05, 79.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 13/120, Train Loss: 1.0681, Validation Loss: 1.0633, Duration: 0:01:19.271752, Best Val Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 14/120 [18:36<2:20:27, 79.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 14/120, Train Loss: 1.0671, Validation Loss: 1.0632, Duration: 0:01:19.109649, Best Val Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñé        | 15/120 [19:56<2:19:31, 79.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/120, Train Loss: 1.0659, Validation Loss: 1.0670, Duration: 0:01:20.243235, Best Val Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 16/120 [21:16<2:18:18, 79.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/120, Train Loss: 1.0653, Validation Loss: 1.0647, Duration: 0:01:19.933400, Best Val Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 17/120 [22:35<2:16:49, 79.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 17/120, Train Loss: 1.0645, Validation Loss: 1.0623, Duration: 0:01:19.485326, Best Val Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 18/120 [23:55<2:15:16, 79.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/120, Train Loss: 1.0639, Validation Loss: 1.0653, Duration: 0:01:19.287899, Best Val Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 19/120 [25:14<2:13:36, 79.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/120, Train Loss: 1.0634, Validation Loss: 1.0624, Duration: 0:01:18.888944, Best Val Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 20/120 [26:32<2:11:44, 79.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/120, Train Loss: 1.0614, Validation Loss: 1.0926, Duration: 0:01:18.291890, Best Val Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 21/120 [27:50<2:10:09, 78.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 21/120, Train Loss: 1.0551, Validation Loss: 1.0346, Duration: 0:01:18.494483, Best Val Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 22/120 [29:10<2:08:59, 78.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 22/120, Train Loss: 1.0203, Validation Loss: 0.9961, Duration: 0:01:19.202196, Best Val Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 23/120 [30:29<2:07:51, 79.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 23/120, Train Loss: 0.9866, Validation Loss: 0.9800, Duration: 0:01:19.341287, Best Val Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 24/120 [31:48<2:06:30, 79.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 24/120, Train Loss: 0.9645, Validation Loss: 0.9616, Duration: 0:01:19.032931, Best Val Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 25/120 [33:07<2:05:06, 79.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 25/120, Train Loss: 0.9475, Validation Loss: 0.9476, Duration: 0:01:18.890493, Best Val Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 26/120 [34:26<2:03:57, 79.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 26/120, Train Loss: 0.9289, Validation Loss: 0.9384, Duration: 0:01:19.364311, Best Val Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñé       | 27/120 [35:47<2:03:12, 79.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/120, Train Loss: 0.9009, Validation Loss: 0.9584, Duration: 0:01:20.357341, Best Val Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 28/120 [37:07<2:02:08, 79.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 28/120, Train Loss: 0.8818, Validation Loss: 0.9374, Duration: 0:01:20.029953, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 29/120 [38:25<2:00:26, 79.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 29/120, Train Loss: 0.8710, Validation Loss: 0.9303, Duration: 0:01:18.834111, Best Val Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 30/120 [39:44<1:58:54, 79.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 30/120, Train Loss: 0.8661, Validation Loss: 0.9220, Duration: 0:01:18.946023, Best Val Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 31/120 [41:03<1:57:15, 79.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 31/120, Train Loss: 0.8597, Validation Loss: 0.9164, Duration: 0:01:18.554451, Best Val Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 32/120 [42:23<1:56:15, 79.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/120, Train Loss: 0.8555, Validation Loss: 0.9200, Duration: 0:01:19.749205, Best Val Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 33/120 [43:43<1:55:18, 79.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 33/120, Train Loss: 0.8489, Validation Loss: 0.9017, Duration: 0:01:20.123079, Best Val Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 34/120 [45:03<1:54:10, 79.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 34/120, Train Loss: 0.8441, Validation Loss: 0.8980, Duration: 0:01:19.968086, Best Val Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 35/120 [46:22<1:52:43, 79.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 35/120, Train Loss: 0.8385, Validation Loss: 0.8939, Duration: 0:01:19.383956, Best Val Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 36/120 [47:42<1:51:22, 79.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 36/120, Train Loss: 0.8349, Validation Loss: 0.8856, Duration: 0:01:19.489589, Best Val Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 37/120 [49:01<1:49:57, 79.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120, Train Loss: 0.8320, Validation Loss: 0.8891, Duration: 0:01:19.342699, Best Val Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 38/120 [50:21<1:48:39, 79.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 38/120, Train Loss: 0.8303, Validation Loss: 0.8825, Duration: 0:01:19.538006, Best Val Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñé      | 39/120 [51:40<1:47:24, 79.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 39/120, Train Loss: 0.8270, Validation Loss: 0.8787, Duration: 0:01:19.710331, Best Val Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 40/120 [52:59<1:45:50, 79.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 40/120, Train Loss: 0.8251, Validation Loss: 0.8781, Duration: 0:01:18.930986, Best Val Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 41/120 [54:19<1:44:29, 79.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/120, Train Loss: 0.8230, Validation Loss: 0.8786, Duration: 0:01:19.337369, Best Val Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 42/120 [55:38<1:43:03, 79.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 42/120, Train Loss: 0.8211, Validation Loss: 0.8756, Duration: 0:01:19.062233, Best Val Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 43/120 [56:58<1:41:59, 79.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/120, Train Loss: 0.8200, Validation Loss: 0.8807, Duration: 0:01:19.958720, Best Val Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 44/120 [58:18<1:40:51, 79.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 44/120, Train Loss: 0.8194, Validation Loss: 0.8736, Duration: 0:01:19.970193, Best Val Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 45/120 [59:36<1:39:13, 79.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 45/120, Train Loss: 0.8175, Validation Loss: 0.8736, Duration: 0:01:18.787249, Best Val Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 46/120 [1:00:55<1:37:37, 79.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 46/120, Train Loss: 0.8160, Validation Loss: 0.8711, Duration: 0:01:18.639574, Best Val Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 47/120 [1:02:14<1:36:16, 79.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120, Train Loss: 0.8144, Validation Loss: 0.8765, Duration: 0:01:19.056073, Best Val Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 48/120 [1:03:33<1:34:53, 79.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120, Train Loss: 0.8135, Validation Loss: 0.8721, Duration: 0:01:18.979207, Best Val Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 49/120 [1:04:53<1:33:47, 79.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 49/120, Train Loss: 0.8126, Validation Loss: 0.8694, Duration: 0:01:19.675289, Best Val Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 50/120 [1:06:12<1:32:33, 79.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/120, Train Loss: 0.8105, Validation Loss: 0.8716, Duration: 0:01:19.498202, Best Val Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51/120 [1:07:32<1:31:23, 79.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/120, Train Loss: 0.8094, Validation Loss: 0.8724, Duration: 0:01:19.772352, Best Val Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 52/120 [1:08:51<1:29:59, 79.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 52/120, Train Loss: 0.8091, Validation Loss: 0.8682, Duration: 0:01:19.281700, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 53/120 [1:10:11<1:28:40, 79.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/120, Train Loss: 0.8081, Validation Loss: 0.8706, Duration: 0:01:19.394860, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 54/120 [1:11:30<1:27:14, 79.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/120, Train Loss: 0.8064, Validation Loss: 0.8795, Duration: 0:01:19.100362, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 55/120 [1:12:49<1:25:56, 79.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/120, Train Loss: 0.8058, Validation Loss: 0.8690, Duration: 0:01:19.384479, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 56/120 [1:14:09<1:24:41, 79.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/120, Train Loss: 0.8046, Validation Loss: 0.8692, Duration: 0:01:19.536842, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 57/120 [1:15:28<1:23:24, 79.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/120, Train Loss: 0.8046, Validation Loss: 0.8685, Duration: 0:01:19.517194, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 58/120 [1:16:48<1:22:04, 79.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120, Train Loss: 0.8035, Validation Loss: 0.8731, Duration: 0:01:19.405230, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 59/120 [1:18:07<1:20:45, 79.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120, Train Loss: 0.8018, Validation Loss: 0.8741, Duration: 0:01:19.456267, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 60/120 [1:19:27<1:19:29, 79.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120, Train Loss: 0.8015, Validation Loss: 0.8710, Duration: 0:01:19.634798, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 61/120 [1:20:46<1:18:07, 79.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120, Train Loss: 0.8005, Validation Loss: 0.8753, Duration: 0:01:19.366377, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 62/120 [1:22:05<1:16:43, 79.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/120, Train Loss: 0.7995, Validation Loss: 0.8699, Duration: 0:01:19.152888, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 63/120 [1:23:24<1:15:21, 79.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/120, Train Loss: 0.7981, Validation Loss: 0.8731, Duration: 0:01:19.237352, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 64/120 [1:24:44<1:14:01, 79.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/120, Train Loss: 0.7982, Validation Loss: 0.8734, Duration: 0:01:19.265396, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 65/120 [1:26:03<1:12:47, 79.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/120, Train Loss: 0.7974, Validation Loss: 0.8705, Duration: 0:01:19.619864, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 66/120 [1:27:22<1:11:22, 79.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120, Train Loss: 0.7969, Validation Loss: 0.8690, Duration: 0:01:19.105158, Best Val Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 67/120 [1:28:41<1:09:55, 79.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (New best validation loss)\n",
      "Epoch 67/120, Train Loss: 0.7964, Validation Loss: 0.8672, Duration: 0:01:18.812330, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 68/120 [1:30:00<1:08:35, 79.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/120, Train Loss: 0.7964, Validation Loss: 0.8685, Duration: 0:01:19.067681, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 69/120 [1:31:19<1:07:14, 79.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/120, Train Loss: 0.7960, Validation Loss: 0.8744, Duration: 0:01:19.069475, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 70/120 [1:32:38<1:05:54, 79.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120, Train Loss: 0.7936, Validation Loss: 0.8714, Duration: 0:01:19.037621, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 71/120 [1:33:57<1:04:31, 79.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120, Train Loss: 0.7930, Validation Loss: 0.8719, Duration: 0:01:18.791316, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 72/120 [1:35:16<1:03:10, 78.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120, Train Loss: 0.7926, Validation Loss: 0.8726, Duration: 0:01:18.893295, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 73/120 [1:36:36<1:02:04, 79.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120, Train Loss: 0.7924, Validation Loss: 0.8707, Duration: 0:01:19.876137, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 74/120 [1:37:55<1:00:43, 79.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/120, Train Loss: 0.7919, Validation Loss: 0.8754, Duration: 0:01:19.153058, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 75/120 [1:39:14<59:24, 79.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/120, Train Loss: 0.7913, Validation Loss: 0.8734, Duration: 0:01:19.165291, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 76/120 [1:40:34<58:06, 79.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/120, Train Loss: 0.7911, Validation Loss: 0.8760, Duration: 0:01:19.306870, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 77/120 [1:41:53<56:51, 79.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/120, Train Loss: 0.7901, Validation Loss: 0.8729, Duration: 0:01:19.587654, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 78/120 [1:43:12<55:26, 79.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/120, Train Loss: 0.7897, Validation Loss: 0.8808, Duration: 0:01:18.892354, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 79/120 [1:44:31<54:03, 79.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120, Train Loss: 0.7889, Validation Loss: 0.8740, Duration: 0:01:18.887108, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 80/120 [1:45:50<52:44, 79.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/120, Train Loss: 0.7892, Validation Loss: 0.8754, Duration: 0:01:19.080822, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 81/120 [1:47:09<51:20, 78.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/120, Train Loss: 0.7883, Validation Loss: 0.8752, Duration: 0:01:18.683674, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 82/120 [1:48:27<49:56, 78.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/120, Train Loss: 0.7879, Validation Loss: 0.8720, Duration: 0:01:18.552654, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 83/120 [1:49:46<48:38, 78.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/120, Train Loss: 0.7875, Validation Loss: 0.8752, Duration: 0:01:18.916909, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 84/120 [1:51:05<47:18, 78.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/120, Train Loss: 0.7873, Validation Loss: 0.8782, Duration: 0:01:18.828229, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 85/120 [1:52:25<46:09, 79.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/120, Train Loss: 0.7864, Validation Loss: 0.8760, Duration: 0:01:19.787337, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 86/120 [1:53:43<44:45, 78.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/120, Train Loss: 0.7858, Validation Loss: 0.8739, Duration: 0:01:18.655831, Best Val Epoch: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 86/120 [1:55:02<45:29, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/120, Train Loss: 0.7845, Validation Loss: 0.8748, Duration: 0:01:18.868023, Best Val Epoch: 66\n",
      "Early stopping triggered after 87 epochs. No improvement for 20 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T16:57:28.072654Z",
     "iopub.status.busy": "2025-02-18T16:57:28.072290Z",
     "iopub.status.idle": "2025-02-18T16:57:43.148483Z",
     "shell.execute_reply": "2025-02-18T16:57:43.147692Z",
     "shell.execute_reply.started": "2025-02-18T16:57:28.072622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-83011c4f4807>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/kaggle/working/best_val_model_pytorch_30')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.7645\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/kaggle/working/best_val_model_pytorch_30')\n",
    "\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T16:57:43.149999Z",
     "iopub.status.busy": "2025-02-18T16:57:43.149681Z",
     "iopub.status.idle": "2025-02-18T16:57:58.453705Z",
     "shell.execute_reply": "2025-02-18T16:57:58.453055Z",
     "shell.execute_reply.started": "2025-02-18T16:57:43.149967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T16:57:58.455094Z",
     "iopub.status.busy": "2025-02-18T16:57:58.454886Z",
     "iopub.status.idle": "2025-02-18T16:57:58.594267Z",
     "shell.execute_reply": "2025-02-18T16:57:58.593619Z",
     "shell.execute_reply.started": "2025-02-18T16:57:58.455077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7644743633860978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6980    0.6138    0.6532     31888\n",
      "           1     0.8061    0.8860    0.8441     78297\n",
      "           2     0.6969    0.6037    0.6469     29303\n",
      "\n",
      "    accuracy                         0.7645    139488\n",
      "   macro avg     0.7337    0.7012    0.7148    139488\n",
      "weighted avg     0.7584    0.7645    0.7591    139488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLob Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T16:58:14.425682Z",
     "iopub.status.busy": "2025-02-18T16:58:14.425329Z",
     "iopub.status.idle": "2025-02-18T16:58:14.439586Z",
     "shell.execute_reply": "2025-02-18T16:58:14.438575Z",
     "shell.execute_reply.started": "2025-02-18T16:58:14.425653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.query_proj   = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.score        = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, query):\n",
    "     \n",
    "\n",
    "        query = query.unsqueeze(1)  \n",
    "\n",
    "        enc_proj = self.encoder_proj(encoder_outputs) \n",
    "        q_proj   = self.query_proj(query)             \n",
    "\n",
    "        energies = torch.tanh(enc_proj + q_proj)\n",
    "\n",
    "        alignment = self.score(energies)\n",
    "        attn_weights = F.softmax(alignment.squeeze(-1), dim=1) \n",
    "\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs) \n",
    "        context = context.squeeze(1) \n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "class deeplobAttention(nn.Module):\n",
    "    def __init__(self, y_len=3, hidden_dim=64, dropout_rate=0.1):\n",
    "  \n",
    "        super().__init__()\n",
    "        self.y_len = y_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # ========== (1) Convolution Blocks ==========\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (1, 2), stride=(1, 2)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 2), stride=(1, 2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 10)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # ========== (2) Inception Modules ==========\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, (1,1), padding='same'),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, (3,1), padding='same'),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, (1,1), padding='same'),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, (5,1), padding='same'),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3,1), stride=(1,1), padding=(1,0)),\n",
    "            nn.Conv2d(32, 64, (1,1), padding='same'),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.attn = BahdanauAttention(hidden_dim=hidden_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, y_len),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (B, 1, 100, 40)  # typical for FI-2010\n",
    "        returns: (B, y_len)  # raw logits\n",
    "        \"\"\"\n",
    "        # 1) CNN stack\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # 2) Inception\n",
    "        x_inp1 = self.inp1(x)\n",
    "        x_inp2 = self.inp2(x)\n",
    "        x_inp3 = self.inp3(x)\n",
    "        x = torch.cat([x_inp1, x_inp2, x_inp3], dim=1)  # => (B, 192, T, 1)\n",
    "\n",
    "        # Prepare for LSTM => (B, T, 192)\n",
    "        x = x.permute(0, 2, 1, 3)  # => (B, T, 192, 1)\n",
    "        x = x.reshape(x.size(0), x.size(1), x.size(2))  # => (B, T, 192)\n",
    "\n",
    "        # 3) LSTM\n",
    "        lstm_out, (h, c) = self.lstm(x)\n",
    "        h_final = h.squeeze(0)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        h_final  = self.dropout(h_final)\n",
    "\n",
    "        # 4) Attention\n",
    "        context, attn_weights = self.attn(lstm_out, h_final)\n",
    "\n",
    "        # 5) Classifier\n",
    "        logits = self.fc(context)  # => (B, y_len)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T16:58:57.873759Z",
     "iopub.status.busy": "2025-02-18T16:58:57.873396Z",
     "iopub.status.idle": "2025-02-18T17:53:48.862336Z",
     "shell.execute_reply": "2025-02-18T17:53:48.860706Z",
     "shell.execute_reply.started": "2025-02-18T16:58:57.873729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [01:26<2:50:57, 86.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 1/120 => Train Loss: 1.0736, Val Loss: 1.0675, Duration: 0:01:26.194963, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 2/120 [02:52<2:49:16, 86.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 2/120 => Train Loss: 1.0699, Val Loss: 1.0668, Duration: 0:01:25.987332, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñé         | 3/120 [04:17<2:47:26, 85.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 => Train Loss: 1.0713, Val Loss: 1.0762, Duration: 0:01:25.626185, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 4/120 [05:43<2:45:50, 85.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 => Train Loss: 1.0696, Val Loss: 1.0694, Duration: 0:01:25.636683, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 5/120 [07:08<2:44:11, 85.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 => Train Loss: 1.0646, Val Loss: 1.0686, Duration: 0:01:25.455120, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 6/120 [08:34<2:42:40, 85.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 => Train Loss: 1.0624, Val Loss: 1.0851, Duration: 0:01:25.542756, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 7/120 [10:00<2:41:19, 85.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 7/120 => Train Loss: 1.0616, Val Loss: 1.0604, Duration: 0:01:25.721876, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 8/120 [11:26<2:40:05, 85.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 => Train Loss: 1.0586, Val Loss: 1.0625, Duration: 0:01:25.983888, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 9/120 [12:52<2:38:47, 85.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 9/120 => Train Loss: 1.0134, Val Loss: 0.9800, Duration: 0:01:25.992541, Best Val Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 10/120 [14:18<2:37:30, 85.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 10/120 => Train Loss: 0.9109, Val Loss: 0.9113, Duration: 0:01:26.083960, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 11/120 [15:43<2:35:52, 85.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 11/120 => Train Loss: 0.8745, Val Loss: 0.8730, Duration: 0:01:25.559337, Best Val Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 12/120 [17:09<2:34:15, 85.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 12/120 => Train Loss: 0.8617, Val Loss: 0.8697, Duration: 0:01:25.466921, Best Val Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 13/120 [18:35<2:33:07, 85.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 => Train Loss: 0.8489, Val Loss: 0.8726, Duration: 0:01:26.233600, Best Val Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 14/120 [20:01<2:31:49, 85.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 14/120 => Train Loss: 0.7958, Val Loss: 0.8572, Duration: 0:01:26.109552, Best Val Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñé        | 15/120 [21:27<2:30:14, 85.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 15/120 => Train Loss: 0.7570, Val Loss: 0.8506, Duration: 0:01:25.648049, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 16/120 [22:52<2:28:39, 85.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 16/120 => Train Loss: 0.7385, Val Loss: 0.8210, Duration: 0:01:25.575835, Best Val Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 17/120 [24:18<2:27:05, 85.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 17/120 => Train Loss: 0.7227, Val Loss: 0.8201, Duration: 0:01:25.502514, Best Val Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 18/120 [25:43<2:25:38, 85.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 18/120 => Train Loss: 0.7097, Val Loss: 0.8066, Duration: 0:01:25.644617, Best Val Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 19/120 [27:09<2:24:03, 85.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 19/120 => Train Loss: 0.6967, Val Loss: 0.8010, Duration: 0:01:25.347203, Best Val Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 20/120 [28:34<2:22:31, 85.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 20/120 => Train Loss: 0.6847, Val Loss: 0.7851, Duration: 0:01:25.356143, Best Val Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 21/120 [30:00<2:21:05, 85.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 => Train Loss: 0.6746, Val Loss: 0.7969, Duration: 0:01:25.491027, Best Val Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 22/120 [31:26<2:19:52, 85.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 22/120 => Train Loss: 0.6665, Val Loss: 0.7818, Duration: 0:01:25.932454, Best Val Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 23/120 [32:52<2:18:41, 85.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 => Train Loss: 0.6582, Val Loss: 0.7837, Duration: 0:01:26.137223, Best Val Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 24/120 [34:18<2:17:24, 85.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 24/120 => Train Loss: 0.6516, Val Loss: 0.7734, Duration: 0:01:26.100133, Best Val Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 25/120 [35:44<2:16:09, 86.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 => Train Loss: 0.6460, Val Loss: 0.7778, Duration: 0:01:26.264383, Best Val Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 26/120 [37:11<2:14:56, 86.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 => Train Loss: 0.6423, Val Loss: 0.7901, Duration: 0:01:26.434993, Best Val Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñé       | 27/120 [38:37<2:13:36, 86.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 => Train Loss: 0.6345, Val Loss: 0.7758, Duration: 0:01:26.357940, Best Val Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 28/120 [40:04<2:12:21, 86.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved (new best validation loss).\n",
      "Epoch 28/120 => Train Loss: 0.6311, Val Loss: 0.7703, Duration: 0:01:26.606681, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 29/120 [41:30<2:11:09, 86.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 => Train Loss: 0.6271, Val Loss: 0.7839, Duration: 0:01:26.860046, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 30/120 [42:57<2:09:46, 86.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 => Train Loss: 0.6223, Val Loss: 0.7752, Duration: 0:01:26.607473, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 31/120 [44:24<2:08:35, 86.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 => Train Loss: 0.6195, Val Loss: 0.7832, Duration: 0:01:27.080379, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 32/120 [45:51<2:07:17, 86.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 => Train Loss: 0.6140, Val Loss: 0.7907, Duration: 0:01:27.040475, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 33/120 [47:18<2:05:53, 86.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 => Train Loss: 0.6103, Val Loss: 0.7825, Duration: 0:01:26.882491, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 34/120 [48:45<2:04:25, 86.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 => Train Loss: 0.6069, Val Loss: 0.7781, Duration: 0:01:26.775253, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 35/120 [50:12<2:03:00, 86.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 => Train Loss: 0.6031, Val Loss: 0.8002, Duration: 0:01:26.869813, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 36/120 [51:38<2:01:31, 86.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 => Train Loss: 0.6005, Val Loss: 0.7982, Duration: 0:01:26.731231, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 37/120 [53:05<2:00:02, 86.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 => Train Loss: 0.5967, Val Loss: 0.7931, Duration: 0:01:26.717971, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 38/120 [54:32<1:58:37, 86.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 => Train Loss: 0.5946, Val Loss: 0.8109, Duration: 0:01:26.849475, Best Val Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 38/120 [54:50<1:58:21, 86.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bdf51d6cb77f>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m train_losses, val_losses = batch_gd(\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-bdf51d6cb77f>\u001b[0m in \u001b[0;36mbatch_gd\u001b[0;34m(model, criterion, optimizer, train_loader, test_loader, epochs, patience)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Backprop + Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mbatch_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             torch._foreach_add_(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs, patience=20):\n",
    "    \"\"\"\n",
    "    Train the model using mini-batch gradient descent with early stopping.\n",
    "    Args:\n",
    "      model, criterion, optimizer: your usual PyTorch objects\n",
    "      train_loader, test_loader: data loaders for training and validation\n",
    "      epochs: max number of epochs to train\n",
    "      patience: how many epochs to wait before early stopping if no improvement\n",
    "\n",
    "    Returns:\n",
    "      train_losses, test_losses: arrays of recorded losses over training\n",
    "    \"\"\"\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        batch_train_losses = []\n",
    "        \n",
    "        # -- Training --\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward => raw logits\n",
    "            outputs = model(inputs.float())   # shape (B, y_len)\n",
    "            # CrossEntropyLoss expects (B, y_len) logits + (B,) class indices\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            \n",
    "            # Backprop + Update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_losses.append(loss.item())\n",
    "\n",
    "        # Mean train loss\n",
    "        train_loss = np.mean(batch_train_losses)\n",
    "\n",
    "        # -- Validation --\n",
    "        model.eval()\n",
    "        batch_test_losses = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs.float())\n",
    "                loss = criterion(outputs, targets.long())\n",
    "                batch_test_losses.append(loss.item())\n",
    "        test_loss = np.mean(batch_test_losses)\n",
    "\n",
    "        # Record\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it]  = test_loss\n",
    "\n",
    "        # Early Stopping logic\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, './best_val_model_pytorch_attention_30')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            no_improve_epochs = 0\n",
    "            print(\"Model saved (new best validation loss).\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f\"Epoch {it+1}/{epochs} => Train Loss: {train_loss:.4f}, Val Loss: {test_loss:.4f}, \"\n",
    "              f\"Duration: {dt}, Best Val Epoch: {best_test_epoch}\")\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping after {it+1} epochs (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    return train_losses[:it+1], test_losses[:it+1]\n",
    "\n",
    "\n",
    "model = deeplobAttention(y_len=3, hidden_dim=64, dropout_rate=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train\n",
    "train_losses, val_losses = batch_gd(\n",
    "    model, criterion, optimizer,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=val_loader, \n",
    "    epochs=120,\n",
    "    patience=17\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:41:24.607214Z",
     "iopub.status.busy": "2025-02-17T21:41:24.606855Z",
     "iopub.status.idle": "2025-02-17T21:41:40.135201Z",
     "shell.execute_reply": "2025-02-17T21:41:40.134315Z",
     "shell.execute_reply.started": "2025-02-17T21:41:24.607185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-914ad1ba3651>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/kaggle/working/best_val_model_pytorch_attention_50')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7813\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/kaggle/working/best_val_model_pytorch_attention_50')\n",
    "model.eval() \n",
    "\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "# Turn off gradient computations for inference\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)  \n",
    "\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# Now compute any metrics (accuracy, F1, etc.)\n",
    "accuracy = np.mean(all_predictions == all_targets)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:41:45.819971Z",
     "iopub.status.busy": "2025-02-17T21:41:45.819668Z",
     "iopub.status.idle": "2025-02-17T21:41:45.872232Z",
     "shell.execute_reply": "2025-02-17T21:41:45.871520Z",
     "shell.execute_reply.started": "2025-02-17T21:41:45.819949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.781257169075476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7419    0.7165    0.7290     38408\n",
      "           1     0.8356    0.8614    0.8483     65996\n",
      "           2     0.7161    0.7014    0.7087     35084\n",
      "\n",
      "    accuracy                         0.7813    139488\n",
      "   macro avg     0.7645    0.7598    0.7620    139488\n",
      "weighted avg     0.7798    0.7813    0.7803    139488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLOB with Self Attention on Order Flow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is in this format: $$ \\text{s}_t^{LOB} := (a_t^1, v_t^{1,a}, b_t^1, v_t^{1,b}, ..., a_t^{10}, v_t^{10,a}, b_t^{10}, v_t^{10,b})^T \\in \\mathbb{R}^{40} $$\n",
    "\n",
    "where $a_t^i$ and $v_t^{i,a}$ are the $i$-th level ask price and ask volume  \n",
    "and $b_t^i$ and $v_t^{i,b}$ are the $i$-th level bid price and bid volume  \n",
    "\n",
    "We will transform it to train our model with Order Flow.\n",
    "We define the *bid order flows* (bOF) and *ask order flows* (aOF) at a timestamp to be 10-variable vectors computed using two consecutive order book states, where each element is given by\n",
    "\n",
    "$$ \\text{bOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      v_t^{i,b}, & b_t^i > b_{t-1}^i \\\\\n",
    "      v_t^{i,b} - v_{t-1}^{i,b}, & b_t^i = b_{t-1}^i \\\\\n",
    "      -v_t^{i,b}, & b_t^i < b_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "$$ \\text{aOF}_{t,i} :=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      -v_t^{i,a}, & a_t^i > a_{t-1}^i \\\\\n",
    "      v_t^{i,a} - v_{t-1}^{i,a}, & a_t^i = a_{t-1}^i \\\\\n",
    "      v_t^{i,a}, & a_t^i < a_{t-1}^i \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "for $i = 1, ..., 10$. With this, we define *order flow* (OF)\n",
    "\n",
    "$$ \\text{OF}_t :=  (\\text{bOF}_{t,1}, \\text{aOF}_{t,1}, ..., \\text{bOF}_{t,10}, \\text{aOF}_{t,10})^T \\in \\mathbb{R}^{20} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    \n",
    "\n",
    "    # Extract bids, asks, and their volumes\n",
    "    bids = df1[:, 2::4]  \n",
    "    bid_volumes = df1[:, 3::4]  \n",
    "    asks = df1[:, 0::4]  \n",
    "    ask_volumes = df1[:, 1::4]  \n",
    "\n",
    "    # Compute changes over time\n",
    "    bid_changes = np.diff(bids, axis=0, prepend=bids[0:1])\n",
    "    ask_changes = np.diff(asks, axis=0, prepend=asks[0:1])\n",
    "    bid_volume_changes = np.diff(bid_volumes, axis=0, prepend=bid_volumes[0:1])\n",
    "    ask_volume_changes = np.diff(ask_volumes, axis=0, prepend=ask_volumes[0:1])\n",
    "\n",
    "    # Compute bOF_t\n",
    "    bOF = np.where(bid_changes > 0, bid_volumes, \n",
    "                np.where(bid_changes == 0, bid_volume_changes, \n",
    "                            -bid_volumes))\n",
    "\n",
    "    # Compute aOF_t\n",
    "    aOF = np.where(ask_changes > 0, -ask_volumes, \n",
    "                np.where(ask_changes == 0, ask_volume_changes, \n",
    "                            ask_volumes))\n",
    "\n",
    "    # Concatenate to get OF_t\n",
    "    OF_t = np.hstack((bOF, aOF))  \n",
    "\n",
    "\n",
    "    return np.array(OF_t)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    \n",
    "    ############## If you have enough RAM, you can modify the dtype of df, dY and dataX to np.float64 ##############\n",
    "    \n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X, dtype=np.float32)\n",
    "\n",
    "    dY = np.array(Y, dtype=np.float32)\n",
    "    dataY = dY[T - 1:N]\n",
    "    dataX = np.zeros((N - T + 1, T, D), dtype=np.float32)\n",
    "\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX, dataY\n",
    "\n",
    "def torch_data(x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    x = torch.unsqueeze(x, 1)\n",
    "    y = torch.from_numpy(y)\n",
    "    y = F.one_hot(y, num_classes=3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, k, num_classes, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "        \n",
    "        x = prepare_x(data)\n",
    "        y = get_label(data)\n",
    "        x, y = data_classification(x, y, self.T)\n",
    "        y = y[:,self.k] - 1\n",
    "\n",
    "        self.length = len(x)\n",
    "        x = torch.from_numpy(x)\n",
    "        self.x = torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please change the data_path to your local path\n",
    "\n",
    "dec_data = np.loadtxt('Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "dec_test1 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_test2 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "dec_test3 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "print(dec_train.shape, dec_val.shape, dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset_train = Dataset(data=dec_train, k=4, num_classes=3, T=100)\n",
    "print(\"1\")\n",
    "dataset_val = Dataset(data=dec_val, k=4, num_classes=3, T=100)\n",
    "print(\"2\")\n",
    "dataset_test = Dataset(data=dec_test, k=4, num_classes=3, T=100)\n",
    "print(\"3\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\n",
    "\n",
    "for x, y in tmp_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.score = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, query):\n",
    "        query = query.unsqueeze(1)\n",
    "        enc_proj = self.encoder_proj(encoder_outputs)  # => (B, seq_len, hidden_dim)\n",
    "        query_proj = self.query_proj(query)            # => (B, 1, hidden_dim)\n",
    "\n",
    "     \n",
    "        energies = torch.tanh(enc_proj + query_proj)\n",
    "        alignment = self.score(energies)   # => (B, seq_len, 1)\n",
    "\n",
    "        attn_weights = F.softmax(alignment.squeeze(-1), dim=1)  # => (B, seq_len)\n",
    "\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        context = context.squeeze(1)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "class DeeplobAttention_OF(nn.Module):\n",
    "    def __init__(self, y_len=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y_len: number of output classes (e.g. 3 for FI-2010 3-label classification).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.y_len = y_len\n",
    "\n",
    "        # ========== 1) Convolution Blocks ==========\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        #since we use Order Flow data, the dimension is divided by 2 and we don't need the second CNN anymore\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # ========== 2) Inception Modules ==========\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3,1), stride=(1,1), padding=(1,0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # ========== 3) LSTM (Encoder) ==========\n",
    "        # After inception, we have 64 + 64 + 64 = 192 channels\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=64,\n",
    "                            num_layers=1, batch_first=True)\n",
    "\n",
    "        # ========== 4) Bahdanau Attention ==========\n",
    "        self.attn = BahdanauAttention(hidden_dim=64)\n",
    "\n",
    "        # ========== 5) Classification Head ==========\n",
    "        self.fc1 = nn.Linear(64, self.y_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, 1, k=100, features=some_value),\n",
    "           typically (B, 1, 100, 40) for the FI-2010 data.\n",
    "        Returns:\n",
    "          forecast_y: (B, y_len)  -> (B, 3) for 3-class classification\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv1(x)  # => (B, 32, ...)\n",
    "        # x = self.conv2(x)  # => (B, 32, ...)\n",
    "        x = self.conv3(x)  # => (B, 32, T, 1)\n",
    "\n",
    "        # -------------- Step 2: Inception --------------\n",
    "        x_inp1 = self.inp1(x)  # => (B, 64, T, 1)\n",
    "        x_inp2 = self.inp2(x)  # => (B, 64, T, 1)\n",
    "        x_inp3 = self.inp3(x)  # => (B, 64, T, 1)\n",
    "\n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)          # => (B, T, 192, 1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2])  # => (B, T, 192)\n",
    "\n",
    "        lstm_out, (h, c) = self.lstm(x)    # lstm_out => (B, T, 64)\n",
    "        final_state = h.squeeze(0)  # => (B, 64)\n",
    "\n",
    "        context, attn_weights = self.attn(lstm_out, final_state)\n",
    "        out = self.fc1(context)           # => (B, y_len)\n",
    "        forecast_y = F.softmax(out, dim=1)  # => (B, y_len)\n",
    "\n",
    "        return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeplobAttention_OF(y_len = dataset_train.num_classes)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DeeplobAttention_v2                      [1, 3]                    --\n",
       "‚îú‚îÄSequential: 1-1                        [1, 32, 94, 10]           --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-1                       [1, 32, 100, 10]          96\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-2                    [1, 32, 100, 10]          --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-3                  [1, 32, 100, 10]          64\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-4                       [1, 32, 97, 10]           4,128\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-5                    [1, 32, 97, 10]           --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-6                  [1, 32, 97, 10]           64\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-7                       [1, 32, 94, 10]           4,128\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-8                    [1, 32, 94, 10]           --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-9                  [1, 32, 94, 10]           64\n",
       "‚îú‚îÄSequential: 1-2                        [1, 32, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-10                      [1, 32, 94, 1]            10,272\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-11                   [1, 32, 94, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-12                 [1, 32, 94, 1]            64\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-13                      [1, 32, 91, 1]            4,128\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-14                   [1, 32, 91, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-15                 [1, 32, 91, 1]            64\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-16                      [1, 32, 88, 1]            4,128\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-17                   [1, 32, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-18                 [1, 32, 88, 1]            64\n",
       "‚îú‚îÄSequential: 1-3                        [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-19                      [1, 64, 88, 1]            2,112\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-20                   [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-21                 [1, 64, 88, 1]            128\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-22                      [1, 64, 88, 1]            12,352\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-23                   [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-24                 [1, 64, 88, 1]            128\n",
       "‚îú‚îÄSequential: 1-4                        [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-25                      [1, 64, 88, 1]            2,112\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-26                   [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-27                 [1, 64, 88, 1]            128\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-28                      [1, 64, 88, 1]            20,544\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-29                   [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-30                 [1, 64, 88, 1]            128\n",
       "‚îú‚îÄSequential: 1-5                        [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄMaxPool2d: 2-31                   [1, 32, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-32                      [1, 64, 88, 1]            2,112\n",
       "‚îÇ    ‚îî‚îÄLeakyReLU: 2-33                   [1, 64, 88, 1]            --\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-34                 [1, 64, 88, 1]            128\n",
       "‚îú‚îÄLSTM: 1-6                              [1, 88, 64]               66,048\n",
       "‚îú‚îÄBahdanauAttention: 1-7                 [1, 64]                   --\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-35                      [1, 88, 64]               4,160\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-36                      [1, 1, 64]                4,160\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-37                      [1, 88, 1]                65\n",
       "‚îú‚îÄLinear: 1-8                            [1, 3]                    195\n",
       "==========================================================================================\n",
       "Total params: 141,764\n",
       "Trainable params: 141,764\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 18.96\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.17\n",
       "Params size (MB): 0.57\n",
       "Estimated Total Size (MB): 2.75\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(model, (1, 1, 100, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs, patience=15):\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "    no_improvement_duration = 0 # counter to stop after 10 epochs without improvement on the validation loss\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "            # print(\"inputs.shape:\", inputs.shape)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # print(\"about to get model output\")\n",
    "            outputs = model(inputs)\n",
    "            # print(\"done getting model output\")\n",
    "            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # print(\"about to optimize\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, './best_val_model_pytorch')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            no_improvement_duration = 0\n",
    "            print('model saved')\n",
    "        else:\n",
    "            no_improvement_duration +=1\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "        \n",
    "        if no_improvement_duration == patience:\n",
    "            print(f\"Validation Loss has not improved for {patience} epochs, the gradient descent will be stopped.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 1/70 [00:55<1:03:25, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 1/70, Train Loss: 0.7801,           Validation Loss: 0.7823, Duration: 0:00:55.157618, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 2/70 [01:49<1:01:40, 54.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 2/70, Train Loss: 0.7064,           Validation Loss: 0.7624, Duration: 0:00:53.910536, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 3/70 [02:39<58:53, 52.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/70, Train Loss: 0.6973,           Validation Loss: 0.7662, Duration: 0:00:50.717157, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 4/70 [03:30<57:11, 51.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 4/70, Train Loss: 0.6906,           Validation Loss: 0.7572, Duration: 0:00:50.847256, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 5/70 [04:21<55:55, 51.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/70, Train Loss: 0.6859,           Validation Loss: 0.7740, Duration: 0:00:50.958081, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñä         | 6/70 [05:13<55:11, 51.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/70, Train Loss: 0.6819,           Validation Loss: 0.7598, Duration: 0:00:51.988967, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 7/70 [06:06<54:36, 52.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 7/70, Train Loss: 0.6786,           Validation Loss: 0.7476, Duration: 0:00:52.570531, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà‚ñè        | 8/70 [07:03<55:20, 53.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 8/70, Train Loss: 0.6746,           Validation Loss: 0.7345, Duration: 0:00:56.871339, Best Val Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 9/70 [07:58<55:00, 54.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/70, Train Loss: 0.6722,           Validation Loss: 0.7352, Duration: 0:00:55.327257, Best Val Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 10/70 [08:51<53:47, 53.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 10/70, Train Loss: 0.6689,           Validation Loss: 0.7291, Duration: 0:00:53.073415, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 11/70 [09:44<52:44, 53.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/70, Train Loss: 0.6662,           Validation Loss: 0.7331, Duration: 0:00:53.305852, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 12/70 [10:39<52:05, 53.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/70, Train Loss: 0.6641,           Validation Loss: 0.7423, Duration: 0:00:54.469354, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñä        | 13/70 [11:35<51:56, 54.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 13/70, Train Loss: 0.6618,           Validation Loss: 0.7255, Duration: 0:00:56.485140, Best Val Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 14/70 [12:32<51:45, 55.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/70, Train Loss: 0.6595,           Validation Loss: 0.7255, Duration: 0:00:57.262325, Best Val Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 15/70 [13:25<49:59, 54.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 15/70, Train Loss: 0.6575,           Validation Loss: 0.7246, Duration: 0:00:52.412612, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 16/70 [14:16<48:16, 53.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/70, Train Loss: 0.6559,           Validation Loss: 0.7303, Duration: 0:00:51.548114, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 17/70 [15:09<47:04, 53.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/70, Train Loss: 0.6535,           Validation Loss: 0.7389, Duration: 0:00:52.486434, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 18/70 [16:01<45:50, 52.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/70, Train Loss: 0.6524,           Validation Loss: 0.7252, Duration: 0:00:51.980982, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 19/70 [16:53<44:45, 52.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/70, Train Loss: 0.6511,           Validation Loss: 0.7266, Duration: 0:00:52.094582, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 20/70 [17:47<44:11, 53.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/70, Train Loss: 0.6500,           Validation Loss: 0.7327, Duration: 0:00:53.917796, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 21/70 [18:39<43:08, 52.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/70, Train Loss: 0.6482,           Validation Loss: 0.7505, Duration: 0:00:52.325412, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà‚ñè      | 22/70 [19:31<42:07, 52.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/70, Train Loss: 0.6471,           Validation Loss: 0.7289, Duration: 0:00:52.248803, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 23/70 [20:29<42:25, 54.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/70, Train Loss: 0.6463,           Validation Loss: 0.7294, Duration: 0:00:57.667650, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 24/70 [21:26<42:11, 55.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/70, Train Loss: 0.6453,           Validation Loss: 0.7273, Duration: 0:00:57.098294, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 25/70 [22:24<41:50, 55.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/70, Train Loss: 0.6443,           Validation Loss: 0.7350, Duration: 0:00:57.509194, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 26/70 [23:23<41:43, 56.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/70, Train Loss: 0.6432,           Validation Loss: 0.7272, Duration: 0:00:59.509066, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñä      | 27/70 [24:20<40:49, 56.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/70, Train Loss: 0.6423,           Validation Loss: 0.7279, Duration: 0:00:57.145925, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 28/70 [25:17<39:47, 56.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/70, Train Loss: 0.6415,           Validation Loss: 0.7278, Duration: 0:00:56.571864, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 29/70 [26:14<38:56, 56.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/70, Train Loss: 0.6407,           Validation Loss: 0.7286, Duration: 0:00:57.285839, Best Val Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 29/70 [27:11<38:26, 56.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/70, Train Loss: 0.6404,           Validation Loss: 0.7291, Duration: 0:00:56.733904, Best Val Epoch: 14\n",
      "Validation Loss has not improved for 15 epochs, the gradient descent will be stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761/1217818421.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('best_val_model_pytorch')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8537\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_val_model_pytorch')\n",
    "\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('best_val_model_pytorch')\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.853707845836201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8497    0.8739    0.8616     47915\n",
      "           1     0.8736    0.8265    0.8494     48050\n",
      "           2     0.8380    0.8615    0.8496     43523\n",
      "\n",
      "    accuracy                         0.8537    139488\n",
      "   macro avg     0.8537    0.8540    0.8535    139488\n",
      "weighted avg     0.8543    0.8537    0.8536    139488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6576084,
     "sourceId": 10620781,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 244809,
     "modelInstanceId": 223047,
     "sourceId": 260893,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 245080,
     "modelInstanceId": 223314,
     "sourceId": 261193,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
